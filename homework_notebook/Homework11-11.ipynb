{
 "metadata": {
  "name": "",
  "signature": "sha256:b70b35abb4cbc33aaf4d3d5ef1289f74c1700a64e196393fd99fbd6ce94a79f8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.spatial.distance import cdist\n",
      "from sklearn.metrics import mean_squared_error as mse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1 Implement the K-Means algorithm. The K-Means technique is an iterative method to find the best K cluster centers of a set of data according to the reconstruction error (i.e. accumulated distances from the real points to their assigned centroids. Each iteration, the algorithm computes the assignemnt of each point to the closest centroid, and then recomputes the centroids based on the points that have been assigned to them.\n",
      "\n",
      "Here is the pseudocode for the K-Means algorithm:\n",
      "1) Initialize the K cluster centers C # To initialize use random values\u00b9 or, alternatively, K random points in the dataset.\n",
      "2) While Y changed during the last iteration do:\n",
      "    Assign each point to the nearest centroid\u00b2.\n",
      "3)     Y[i] = nearest_centroid(X[i], C)\n",
      "    Recompute each centroid as the mean of the points assigned to it\u00b2.\n",
      "4)     C = recompute_centroids(Y, X)\n",
      "5) Return C #Final stable centroids.\n",
      "\n",
      "    \u00b9 To avoid degenerate solutions, make sure that the random values are in the range taken by the components of the data set vectors.\n",
      "    \u00b2 Nearest_centroid and recompute_centroids have to compute respectively:\n",
      "    Nearest centroid\n",
      "    Update centroids\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def KMeans(data, k, dim):\n",
      "    C = np.random.random((k,dim)) * X.max()\n",
      "    old_C = np.zeros((K, dim)) \n",
      "    maxiter = 100\n",
      "    ii=0\n",
      "    \n",
      "    while (C != old_C).all() and ii<maxiter:\n",
      "        old_C = np.copy(C)\n",
      "        clusters = np.zeros((k,X.shape[0]))\n",
      "        d_min = np.zeros((k,X.shape[0]))\n",
      "        clusters = cdist(C,X)\n",
      "        d_min = np.argmin(clusters,0)\n",
      "        C = np.array([np.mean(X[d_min == j, :],0) for j in range(k)])\n",
      "        ii += 1\n",
      "       \n",
      "    #Al posar un m\u00e1xim d'iteracions, per evitar errors si no aconseguim trobar els centroides els\n",
      "    # forcem a als primers valors de data.\n",
      "    if (C != old_C).all() and ii>=maxiter:\n",
      "        C = data[:k,:dim]\n",
      "       \n",
      "    return C, d_min"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q2 Try the implemented K-Means algorithm with K=3 in the first two dimensions of the iris dataset, so you can visualize it. Then try it with the full four-dimensional vectors and report the MSE for each cluster and globally. Optional: Compare how far are the clusters from the actual center of each class (i.e. how well the clusters predict the classes)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 3\n",
      "data = np.loadtxt(\"iris.data\",delimiter = ',',usecols=range(0,4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"First two dimensions of the iris dataset\"\n",
      "print\n",
      "\n",
      "dim = 2\n",
      "X = data[:,:dim]\n",
      "C,d_min = KMeans(X, K, dim)\n",
      "print \"Final stable centroids are: \"\n",
      "print C\n",
      "print\n",
      "\n",
      "y_pred = np.array([C[d_min[i],:] for i in range(data.shape[0])]) \n",
      "\n",
      "print \"Global MSE is:\", mse(X,y_pred)\n",
      "\n",
      "\n",
      "for j in range(K):\n",
      "    \n",
      "    if X[d_min==j,:].any():\n",
      "        print \"MSE Cluster\", j, \"is:\", mse(X[d_min==j,:],y_pred[d_min == j,:])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Full four-dimensions of the iris dataset\"\n",
      "print\n",
      "\n",
      "dim = 4\n",
      "X = data[:,:dim]\n",
      "C, d_min = KMeans(X, K, dim)\n",
      "print \"Final stable centroids are: \"\n",
      "print C\n",
      "print\n",
      "\n",
      "y_pred = np.array([C[d_min[i],:] for i in range(data.shape[0])]) \n",
      "\n",
      "print \"Global MSE is:\", mse(X,y_pred)\n",
      "\n",
      "for j in range(K):\n",
      "    \n",
      "    if X[d_min==j,:].any():\n",
      "        print \"MSE Cluster\", j, \"is:\", mse(X[d_min==j,:],y_pred[d_min == j,:])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}